{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35350243-82e6-4586-ac08-f1b5877657ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Agastya Singh\\AppData\\Local\\Temp\\ipykernel_12784\\4291339300.py:12: DtypeWarning: Columns (30,33,34,50,62,67,68,98,99,100) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sod_india = pd.read_csv(r\"C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\Secondary Order Dump (New)_india.csv\")\n",
      "C:\\Users\\Agastya Singh\\venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Conditional Formatting extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl \n",
    "import datetime\n",
    "\n",
    "user_india = pd.read_excel(r\"C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\Attendance Report (New)_india.xlsx\") \n",
    "user_nepal = pd.read_excel(r\"C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\Attendance Report (New)_nepal.xlsx\")\n",
    "\n",
    "master_position_india=pd.read_excel(r\"C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\Master Data (Position)_india.xlsx\") \n",
    "master_position_nepal=pd.read_excel(r\"C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\Master Data (Position)_nepal.xlsx\")\n",
    "\n",
    "#sod_india = pd.read_excel(r\"C:\\Users\\Admin\\Downloads\\Secondary Order Dump.xlsx\") \n",
    "sod_india = pd.read_csv(r\"C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\Secondary Order Dump (New)_india.csv\") \n",
    "sod_nepal = pd.read_excel(r\"C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\Secondary Order Dump(New)_nepal.xlsx\")\n",
    "\n",
    "sod_nepal.columns = sod_nepal.columns.str.strip()\n",
    "sod_india.columns = sod_india.columns.str.strip()\n",
    "\n",
    "user = pd.concat([user_india, user_nepal], ignore_index=True)\n",
    "master_position = pd.concat([master_position_india, master_position_nepal], ignore_index=True)\n",
    "master_position_s3 = pd.concat([master_position_india, master_position_nepal], ignore_index=True)\n",
    "\n",
    "master_position=master_position.drop_duplicates(subset='Beats Erp Id')\n",
    "\n",
    "sod = pd.concat([sod_india, sod_nepal], ignore_index=True)\n",
    "\n",
    "#db_master = pd.read_excel(r\"C:\\Users\\Shree\\OneDrive - Niine Pvt. Ltd\\Documents\\Shreemala\\SR_Tracker_Code\\upload_filesuct Master\\DB Master.xlsx\",sheet_name='DB Master')\n",
    "\n",
    "target = pd.read_excel(r\"C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\India_Target_Template - Nov'24.xlsx\",sheet_name='DSR & USR Tgts',header=3)\n",
    "\n",
    "target_territory = pd.read_excel(r\"C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\India_Target_Template - Nov'24.xlsx\",sheet_name='Territory Tgts',header=4)\n",
    "target_zone = pd.read_excel(r\"C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\India_Target_Template - Nov'24.xlsx\",sheet_name='ZoneTargets',header=2) \n",
    "target_region = pd.read_excel(r\"C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\India_Target_Template - Nov'24.xlsx\",sheet_name='Region Tgts',header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bca5b4e-bb3d-4311-ad0a-c10f2e467623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['02-11-2024', '01-11-2024', '04-11-2024', '03-11-2024',\n",
       "       '05-11-2024', '06-11-2024', '07-11-2024', '08-11-2024',\n",
       "       '09-11-2024', '10-11-2024', '11-11-2024', '12-11-2024',\n",
       "       '13-11-2024', '14-11-2024', '15-11-2024', '16-11-2024',\n",
       "       '18-11-2024', '17-11-2024', Timestamp('2024-11-04 00:00:00'),\n",
       "       Timestamp('2024-11-03 00:00:00'), Timestamp('2024-11-06 00:00:00'),\n",
       "       Timestamp('2024-11-05 00:00:00'), Timestamp('2024-11-08 00:00:00'),\n",
       "       Timestamp('2024-11-07 00:00:00'), Timestamp('2024-11-10 00:00:00'),\n",
       "       Timestamp('2024-11-12 00:00:00'), Timestamp('2024-11-11 00:00:00'),\n",
       "       Timestamp('2024-11-13 00:00:00'), Timestamp('2024-11-14 00:00:00'),\n",
       "       Timestamp('2024-11-15 00:00:00'), Timestamp('2024-11-17 00:00:00'),\n",
       "       Timestamp('2024-11-18 00:00:00')], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sod['Order Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11718c29-e4c3-490d-989b-b69c5e320080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(date):\n",
    "    if isinstance(date, pd.Timestamp):\n",
    "        return date.strftime('%d-%m-%Y')\n",
    "    elif isinstance(date, str):\n",
    "        return date  # Assume it's already in the correct format\n",
    "    else:\n",
    "        return None  # Handle unexpected types if necessary\n",
    "\n",
    "# Apply the function to the 'order_date' column\n",
    "sod['Order Date'] = sod['Order Date'].apply(format_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e51552b8-29a1-4a65-beb4-3e4d6c403069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users=user[(user['User Designation'] == 'USR') | (user['User Designation'] == 'DSR') | (user['User Position Level'] == 'L1Position') | (user['User Position Level'] == 'L2Position') ] \n",
    "users = users.rename(columns=lambda x: x.strip())\n",
    "users=users[['Level 5 Position','Level 4 Position','L4Position User','Level 3 Position','L3Position User','Salesman ErpId', 'User', 'User Status','Date of Joining']] \n",
    "users = users.rename(columns={'Level 3 Position': 'L3 Position Name'})\n",
    "users = users.rename(columns={'Salesman ErpId': 'User Erp Id'})\n",
    "\n",
    "master_position = master_position.rename(columns={'Zone': 'Regions'})\n",
    "master_position = master_position.rename(columns={'Region': 'Zone'})\n",
    "master_position = master_position.rename(columns=lambda x: x.strip())\n",
    "master_position=master_position[['L3 Position Name','L2 Position Name','L1 Position Name','User Erp Id','Zone','Regions','Territory','Beats','Beats Erp Id','SuperStockist Erp Id','SuperStockist Name','Distributor Name', 'Distributor Erp Id','Count of Outlets']]\n",
    "\n",
    "se_level_master=master_position[['Zone','Regions','Territory','L3 Position Name']] \n",
    "se_level_master=se_level_master.drop_duplicates(subset='L3 Position Name')\n",
    "sr_level_master=master_position[['L1 Position Name','User Erp Id','Beats','Beats Erp Id','SuperStockist Erp Id','SuperStockist Name','Distributor Name', 'Distributor Erp Id','Count of Outlets']]\n",
    "#sr_level_master=sr_level_master.drop_duplicates(subset='User Erp Id')\n",
    "\n",
    "sr_master = pd.merge(users, se_level_master, on='L3 Position Name', how='left')\n",
    "sr_master_nan_territory = sr_master[sr_master['L3 Position Name'].isna()]\n",
    "\n",
    "sr_master_nan_territory=sr_master_nan_territory[['Zone','Regions','Territory','User Erp Id']]\n",
    "sr_master_nan_territory = sr_master_nan_territory.drop_duplicates()\n",
    "se_level_master=master_position[['Zone','Regions','Territory','User Erp Id']]\n",
    "se_level_master = se_level_master.drop_duplicates()\n",
    "sr_master_nan_territory = pd.merge(sr_master_nan_territory, se_level_master, on='User Erp Id', how='left')\n",
    "sr_master_nan_territory=sr_master_nan_territory[['Zone_y','Regions_y','Territory_y','User Erp Id']]\n",
    "sr_master_nan_territory = sr_master_nan_territory.drop_duplicates()\n",
    "\n",
    "sr_master_nan_territory = sr_master_nan_territory.dropna() \n",
    "sr_master = pd.merge(sr_master, sr_master_nan_territory, on='User Erp Id', how='left')\n",
    "sr_master.loc[sr_master['L3 Position Name'].isna(), 'Territory'] = sr_master['Territory_y']\n",
    "sr_master.loc[sr_master['L3 Position Name'].isna(), 'Zone'] = sr_master['Zone_y']\n",
    "sr_master.loc[sr_master['L3 Position Name'].isna(), 'Regions'] = sr_master['Regions_y']\n",
    "\n",
    "sr_master = pd.merge(sr_master, sr_level_master, on='User Erp Id', how='outer')\n",
    "#sr_master.to_excel(r'C:\\Users\\Admin\\Downloads\\sr_master.xlsx')\n",
    "#sr_master = sr_master.dropna(subset=['Territory'])\n",
    "sod = sod.drop_duplicates()\n",
    "sod = sod.rename(columns={'ESM Erp Id': 'Erp Code'})\n",
    "\n",
    "\n",
    "beat_retailing = sod.groupby(['Erp Code','Beat Erp Id'])['Order Date'].nunique().reset_index()\n",
    "beat_tc = sod.groupby(['Beat Erp Id'])['Order No'].nunique().reset_index()\n",
    "beat_utc = sod.groupby(['Beat Erp Id'])['Outlet Erp Id'].nunique().reset_index()\n",
    "condition = sod['Net Value'] > 0\n",
    "sod_prod = sod[condition]\n",
    "beat_pc = sod_prod.groupby(['Beat Erp Id'])['Order No'].nunique().reset_index()\n",
    "beat_upc = sod_prod.groupby(['Beat Erp Id'])['Outlet Erp Id'].nunique().reset_index()\n",
    "beat_lpc = sod_prod.groupby(['Beat Erp Id'])['Product ErpId'].count().reset_index()\n",
    "\n",
    "\n",
    "napkin = sod[sod['PrimaryCategory'] == 'Sanitary Pads']\n",
    "diaper = sod[sod['PrimaryCategory'] == 'Diaper']\n",
    "\n",
    "beat_napkin_ach = napkin.groupby(['Beat Erp Id'])['Net Value'].sum().reset_index()\n",
    "beat_diaper_ach = diaper.groupby(['Beat Erp Id'])['Net Value'].sum().reset_index()\n",
    "beat_napkin_eco = napkin.groupby(['Beat Erp Id'])['Order No'].nunique().reset_index()\n",
    "beat_diaper_eco = diaper.groupby(['Beat Erp Id'])['Order No'].nunique().reset_index()\n",
    "\n",
    "beat_total_ach = sod.groupby(['Beat Erp Id'])['Net Value'].sum().reset_index()\n",
    "\n",
    "eco_dc = sod[sod['SecondaryCategory'] == 'Dry Comfort']\n",
    "eco_ns = sod[sod['SecondaryCategory'] == 'Naturally Soft']\n",
    "eco_ultra = sod[sod['SecondaryCategory'] == 'Ultra']\n",
    "eco_ultrapremium = sod[sod['SecondaryCategory'] == 'Ultra Premium']\n",
    "eco_babywipes = sod[sod['SecondaryCategory'] == 'Baby Wipes']\n",
    "\n",
    "eco_dc_beat = eco_dc.groupby(['Beat Erp Id'])['Outlet Erp Id'].nunique().reset_index()\n",
    "eco_ns_beat = eco_ns.groupby(['Beat Erp Id'])['Outlet Erp Id'].nunique().reset_index()\n",
    "eco_ultra_beat = eco_ultra.groupby(['Beat Erp Id'])['Outlet Erp Id'].nunique().reset_index()\n",
    "eco_ultrapremium_beat = eco_ultrapremium.groupby(['Beat Erp Id'])['Outlet Erp Id'].nunique().reset_index()\n",
    "eco_babywipes_beat = eco_babywipes.groupby(['Beat Erp Id'])['Outlet Erp Id'].nunique().reset_index()\n",
    "\n",
    "\n",
    "beat_tc = beat_tc.rename(columns={'Order No': 'TC'})\n",
    "beat_utc = beat_utc.rename(columns={'Outlet Erp Id': 'UTC'})\n",
    "beat_pc = beat_pc.rename(columns={'Order No': 'PC'})\n",
    "beat_upc = beat_upc.rename(columns={'Outlet Erp Id': 'UPC'})\n",
    "beat_lpc = beat_lpc.rename(columns={'Product ErpId': 'LPC'})\n",
    "beat_napkin_ach = beat_napkin_ach.rename(columns={'Net Value': 'Napkin_ach'})\n",
    "beat_diaper_ach = beat_diaper_ach.rename(columns={'Net Value': 'Diaper_ach'})\n",
    "beat_napkin_eco = beat_napkin_eco.rename(columns={'Order No': 'Napkin_eco'})\n",
    "beat_diaper_eco = beat_diaper_eco.rename(columns={'Order No': 'Diaper_eco'})\n",
    "eco_dc_beat = eco_dc_beat.rename(columns={'Outlet Erp Id': 'dc_eco'})\n",
    "eco_ns_beat = eco_ns_beat.rename(columns={'Outlet Erp Id': 'ns_eco'})\n",
    "eco_ultra_beat = eco_ultra_beat.rename(columns={'Outlet Erp Id': 'ultra_eco'})\n",
    "eco_ultrapremium_beat = eco_ultrapremium_beat.rename(columns={'Outlet Erp Id': 'ultrapremium_eco'})\n",
    "eco_babywipes_beat = eco_babywipes_beat.rename(columns={'Outlet Erp Id': 'babywipes_eco'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a29a028b-1a31-424e-9913-ca2824b8e80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Level 5 Position', 'Level 4 Position', 'L4Position User',\n",
       "       'L3 Position Name', 'L3Position User', 'User Erp Id', 'User',\n",
       "       'User Status', 'Date of Joining', 'Zone', 'Regions', 'Territory',\n",
       "       'Zone_y', 'Regions_y', 'Territory_y', 'L1 Position Name', 'Beats',\n",
       "       'Beats Erp Id', 'SuperStockist Erp Id', 'SuperStockist Name',\n",
       "       'Distributor Name', 'Distributor Erp Id', 'Count of Outlets'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_master.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1788397b-e587-4770-9392-54e9a29f738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sr_master = sr_master.rename(columns={'User Erp Id': 'Erp Code'})\n",
    "sr_master = sr_master.rename(columns={'Beats Erp Id': 'Beat Erp Id'})\n",
    "\n",
    "beat_retailing = beat_retailing.drop(columns=['Beat Erp Id'])\n",
    "beat_retailing=beat_retailing.drop_duplicates(subset='Erp Code')\n",
    "beat_summary = pd.merge(sr_master, beat_retailing, on='Erp Code', how='left')\n",
    "beat_summary = pd.merge(beat_summary, beat_tc, on='Beat Erp Id', how='left')\n",
    "beat_summary = pd.merge(beat_summary, beat_utc, on='Beat Erp Id', how='left')\n",
    "beat_summary = pd.merge(beat_summary, beat_pc, on='Beat Erp Id', how='left')\n",
    "beat_summary = pd.merge(beat_summary, beat_upc, on='Beat Erp Id', how='left')\n",
    "beat_summary = pd.merge(beat_summary, beat_lpc, on='Beat Erp Id', how='left')\n",
    "beat_summary = pd.merge(beat_summary, beat_napkin_ach, on='Beat Erp Id', how='left')\n",
    "beat_summary = pd.merge(beat_summary, beat_diaper_ach, on='Beat Erp Id', how='left')\n",
    "beat_summary = pd.merge(beat_summary, beat_total_ach, on='Beat Erp Id', how='left')\n",
    "beat_summary = pd.merge(beat_summary, eco_dc_beat, on='Beat Erp Id', how='left')\n",
    "beat_summary = pd.merge(beat_summary, eco_ns_beat, on='Beat Erp Id', how='left')\n",
    "beat_summary = pd.merge(beat_summary, eco_ultra_beat, on='Beat Erp Id', how='left')\n",
    "beat_summary = pd.merge(beat_summary, eco_ultrapremium_beat, on='Beat Erp Id', how='left')\n",
    "beat_summary = pd.merge(beat_summary, eco_babywipes_beat, on='Beat Erp Id', how='left')\n",
    "\n",
    "beat_summary = pd.merge(beat_summary, beat_napkin_eco, on='Beat Erp Id', how='left')\n",
    "beat_summary = pd.merge(beat_summary, beat_diaper_eco, on='Beat Erp Id', how='left')\n",
    "\n",
    "\n",
    "###count weekdays till today####\n",
    "def count_working_days_until_today():\n",
    "    working_days = 0\n",
    "    today = datetime.date.today()\n",
    "    current_date = datetime.date(today.year, today.month, 1)  # First day of the month\n",
    "    while current_date <= today:\n",
    "        if current_date.weekday() != 5:  # Saturday\n",
    "            working_days += 1\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "    return working_days\n",
    "\n",
    "number_of_working_days = count_working_days_until_today()-1\n",
    "\n",
    "###count total weedays in current month #####\n",
    "def count_weekdays_in_current_month():\n",
    "    weekdays = 0\n",
    "    today = datetime.date.today()\n",
    "    current_date = datetime.date(today.year, today.month, 1)  # First day of the month\n",
    "    while current_date.month == today.month:\n",
    "        if current_date.weekday() != 5:  # Saturday\n",
    "            weekdays += 1\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "    return weekdays\n",
    "\n",
    "number_of_weekdays = count_weekdays_in_current_month()\n",
    "balance_days=number_of_weekdays-number_of_working_days\n",
    "\n",
    "beat_summary['Napkin_ach'] = beat_summary['Napkin_ach'] / 100000\n",
    "beat_summary['Diaper_ach'] = beat_summary['Diaper_ach'] / 100000\n",
    "beat_summary['Net Value'] = beat_summary['Net Value'] / 100000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cb3319-63a6-48de-9e15-bc70a4253005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_divide(x, y):\n",
    "    try:\n",
    "        return x / y\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return 0\n",
    "        \n",
    "beat_summary['UTC%'] = beat_summary.apply(lambda row: safe_divide(row['UTC'], row['Count of Outlets']), axis=1)\n",
    "beat_summary['UPC%'] = beat_summary.apply(lambda row: safe_divide(row['UPC'], row['Count of Outlets']), axis=1)\n",
    "beat_summary['Average Order value'] = beat_summary.apply(lambda row: safe_divide(row['Net Value'], row['PC']), axis=1)\n",
    "beat_summary['LPC_value'] = beat_summary.apply(lambda row: safe_divide(row['LPC'], row['PC']), axis=1)                                                        \n",
    "#sr_summary['TotalAch%'] = sr_summary.apply(lambda row: safe_divide(row['Net Value'], row['FA_total_target']), axis=1)\n",
    "beat_summary['DRR'] = beat_summary.apply(lambda row: safe_divide(row['Net Value'], number_of_working_days), axis=1)\n",
    "#sr_summary['RRR'] = sr_summary.apply(lambda row: safe_divide(row['rrr_diff'], balance_days), axis=1)\n",
    "beat_summary['napkin_eco%'] = beat_summary.apply(lambda row: safe_divide(row['Napkin_eco'], row['Count of Outlets']), axis=1)\n",
    "beat_summary['diaper_eco%'] = beat_summary.apply(lambda row: safe_divide(row['Diaper_eco'], row['Count of Outlets']), axis=1)                                                        \n",
    "beat_summary['Average Order value'] *= 100000\n",
    "beat_summary['Diaper_ach'] = beat_summary['Diaper_ach'].round(2)\n",
    "beat_summary['Napkin_ach'] = beat_summary['Napkin_ach'].round(2)\n",
    "beat_summary['UTC%'] = beat_summary['UTC%'].round(2)\n",
    "beat_summary['UPC%'] = beat_summary['UPC%'].round(2)\n",
    "#sr_summary['rrr_diff'] = sr_summary['rrr_diff'].round(2)\n",
    "#sr_summary['TotalAch%'] = sr_summary['TotalAch%'].round(2)\n",
    "beat_summary['DRR'] = beat_summary['DRR'].round(2)\n",
    "#sr_summary['RRR'] = sr_summary['RRR'].round(2)\n",
    "beat_summary['napkin_eco%'] = beat_summary['napkin_eco%'].round(2)\n",
    "beat_summary['diaper_eco%'] = beat_summary['diaper_eco%'].round(2)\n",
    "beat_summary['Napkin_ach'] = beat_summary['Napkin_ach'].round(2)\n",
    "\n",
    "beat_summary['Average Order value'] = beat_summary['Average Order value'].round(2)\n",
    "beat_summary = beat_summary.dropna(subset=['Erp Code'])\n",
    "beat_summary = beat_summary.dropna(subset=['User'])\n",
    "\n",
    "weekdays = count_working_days_until_today()-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56f4990-60c9-428e-9bda-fb1401c0932a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Agastya Singh\\AppData\\Local\\Temp\\ipykernel_12784\\4194916014.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sr_master2['Count of Outlets'] = sr_master2['Count of Outlets'].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sr_sod=sod[['Order No','Order Date',\t'Outlet Erp Id',\t'Net Value',\t'PrimaryCategory',\t'Erp Code',\t'SecondaryCategory','Product ErpId']]\n",
    "\n",
    "sr_master['User Status'] = sr_master['User Status'].fillna('Active')\n",
    "sr_master2=sr_master[['Erp Code','User','User Status',\t'Date of Joining',\t'Zone',\t'Regions',\t'Territory','Count of Outlets']]\n",
    "sr_master2['Count of Outlets'] = sr_master2['Count of Outlets'].fillna(0)\n",
    "sr_master_outlets = sr_master2.groupby(['Erp Code','User',\t'User Status',\t'Date of Joining',\t'Zone',\t'Regions',\t'Territory'])['Count of Outlets'].sum().reset_index()\n",
    "\n",
    "sr_retailing = sr_sod.groupby(['Erp Code'])['Order Date'].nunique().reset_index()\n",
    "sr_tc = sr_sod.groupby(['Erp Code'])['Order No'].nunique().reset_index()\n",
    "sr_utc = sr_sod.groupby(['Erp Code'])['Outlet Erp Id'].nunique().reset_index()\n",
    "condition = sr_sod['Net Value'] > 0\n",
    "sod_prod = sr_sod[condition]\n",
    "sr_pc = sod_prod.groupby(['Erp Code'])['Order No'].nunique().reset_index()\n",
    "sr_upc = sod_prod.groupby(['Erp Code'])['Outlet Erp Id'].nunique().reset_index()\n",
    "sr_lpc = sod_prod.groupby(['Erp Code'])['Product ErpId'].count().reset_index()\n",
    "\n",
    "\n",
    "sr_napkin = sr_sod[sr_sod['PrimaryCategory'] == 'Sanitary Pads']\n",
    "sr_diaper = sr_sod[sr_sod['PrimaryCategory'] == 'Diaper']\n",
    "\n",
    "sr_napkin_ach = sr_napkin.groupby(['Erp Code'])['Net Value'].sum().reset_index()\n",
    "sr_diaper_ach = sr_diaper.groupby(['Erp Code'])['Net Value'].sum().reset_index()\n",
    "sr_napkin_eco = sr_napkin.groupby(['Erp Code'])['Order No'].nunique().reset_index()\n",
    "sr_diaper_eco = sr_diaper.groupby(['Erp Code'])['Order No'].nunique().reset_index()\n",
    "\n",
    "sr_total_ach = sr_sod.groupby(['Erp Code'])['Net Value'].sum().reset_index()\n",
    "\n",
    "sr_eco_dc = sr_sod[sr_sod['SecondaryCategory'] == 'Dry Comfort']\n",
    "sr_eco_ns = sr_sod[sr_sod['SecondaryCategory'] == 'Naturally Soft']\n",
    "sr_eco_ultra = sr_sod[sr_sod['SecondaryCategory'] == 'Ultra']\n",
    "sr_eco_ultrapremium = sr_sod[sr_sod['SecondaryCategory'] == 'Ultra Premium']\n",
    "sr_eco_babywipes = sr_sod[sr_sod['SecondaryCategory'] == 'Baby Wipes']\n",
    "\n",
    "sr_eco_dc_beat = sr_eco_dc.groupby(['Erp Code'])['Outlet Erp Id'].nunique().reset_index()\n",
    "sr_eco_ns_beat = sr_eco_ns.groupby(['Erp Code'])['Outlet Erp Id'].nunique().reset_index()\n",
    "sr_eco_ultra_beat = sr_eco_ultra.groupby(['Erp Code'])['Outlet Erp Id'].nunique().reset_index()\n",
    "sr_eco_ultrapremium_beat = sr_eco_ultrapremium.groupby(['Erp Code'])['Outlet Erp Id'].nunique().reset_index()\n",
    "sr_co_babywipes= sr_eco_babywipes.groupby(['Erp Code'])['Outlet Erp Id'].nunique().reset_index()\n",
    "\n",
    "\n",
    "sr_tc = sr_tc.rename(columns={'Order No': 'TC'})\n",
    "sr_utc = sr_utc.rename(columns={'Outlet Erp Id': 'UTC'})\n",
    "sr_pc = sr_pc.rename(columns={'Order No': 'PC'})\n",
    "sr_upc = sr_upc.rename(columns={'Outlet Erp Id': 'UPC'})\n",
    "sr_lpc = sr_lpc.rename(columns={'Product ErpId': 'LPC'})\n",
    "sr_napkin_ach = sr_napkin_ach.rename(columns={'Net Value': 'Napkin_ach'})\n",
    "sr_diaper_ach = sr_diaper_ach.rename(columns={'Net Value': 'Diaper_ach'})\n",
    "sr_napkin_eco = sr_napkin_eco.rename(columns={'Order No': 'Napkin_eco'})\n",
    "sr_diaper_eco = sr_diaper_eco.rename(columns={'Order No': 'Diaper_eco'})\n",
    "sr_eco_dc_beat = sr_eco_dc_beat.rename(columns={'Outlet Erp Id': 'dc_eco'})\n",
    "sr_eco_ns_beat = sr_eco_ns_beat.rename(columns={'Outlet Erp Id': 'ns_eco'})\n",
    "sr_eco_ultra_beat = sr_eco_ultra_beat.rename(columns={'Outlet Erp Id': 'ultra_eco'})\n",
    "sr_eco_ultrapremium_beat = sr_eco_ultrapremium_beat.rename(columns={'Outlet Erp Id': 'ultrapremium_eco'})\n",
    "sr_eco_babywipes = sr_eco_babywipes.rename(columns={'Outlet Erp Id': 'babywipes_eco'})\n",
    "\n",
    "users = users.rename(columns={'User Erp Id': 'Erp Code'})\n",
    "\n",
    "sr_summary = pd.merge(sr_master_outlets, sr_retailing, on='Erp Code', how='left')\n",
    "sr_summary = pd.merge(sr_summary, sr_tc, on='Erp Code', how='left')\n",
    "sr_summary = pd.merge(sr_summary, sr_utc, on='Erp Code', how='left')\n",
    "sr_summary = pd.merge(sr_summary, sr_pc, on='Erp Code', how='left')\n",
    "sr_summary = pd.merge(sr_summary, sr_upc, on='Erp Code', how='left')\n",
    "sr_summary = pd.merge(sr_summary, sr_lpc, on='Erp Code', how='left')\n",
    "sr_summary = pd.merge(sr_summary, sr_napkin_ach, on='Erp Code', how='left')\n",
    "sr_summary = pd.merge(sr_summary, sr_diaper_ach, on='Erp Code', how='left')\n",
    "sr_summary = pd.merge(sr_summary, sr_total_ach, on='Erp Code', how='left')\n",
    "sr_summary = pd.merge(sr_summary, sr_eco_dc_beat, on='Erp Code', how='left')\n",
    "sr_summary = pd.merge(sr_summary, sr_eco_ns_beat, on='Erp Code', how='left')\n",
    "sr_summary = pd.merge(sr_summary, sr_eco_ultra_beat, on='Erp Code', how='left')\n",
    "sr_summary = pd.merge(sr_summary, sr_eco_ultrapremium_beat, on='Erp Code', how='left')\n",
    "sr_summary = pd.merge(sr_summary, sr_eco_babywipes, on='Erp Code', how='left')\n",
    "\n",
    "sr_summary = pd.merge(sr_summary, sr_napkin_eco, on='Erp Code', how='left')\n",
    "sr_summary = pd.merge(sr_summary, sr_diaper_eco, on='Erp Code', how='left')\n",
    "\n",
    "sr_summary=sr_summary[['Erp Code', 'User', 'User Status', 'Date of Joining', 'Zone', 'Regions',\n",
    "       'Territory', 'Count of Outlets', 'Order Date_x', 'TC', 'UTC', 'PC',\n",
    "       'UPC','LPC', 'Napkin_ach', 'Diaper_ach', 'Net Value_x', 'dc_eco', 'ns_eco',\n",
    "       'ultra_eco', 'ultrapremium_eco', 'Order No','Napkin_eco', 'Diaper_eco']]\n",
    "\n",
    "sr_summary['Napkin_ach'] = sr_summary['Napkin_ach'] / 100000\n",
    "sr_summary['Diaper_ach'] = sr_summary['Diaper_ach'] / 100000\n",
    "\n",
    "target = target.rename(columns={'Target': 'FA_napkin_target'})\n",
    "target = target.rename(columns={'Target.1': 'FA_diaper_target'})\n",
    "target = target.rename(columns={'Target.2': 'FA_total_target'})\n",
    "target = target.rename(columns={'Target.3': 'SNS_napkin_target'})\n",
    "target = target.rename(columns={'Target.4': 'SNS_diaper_target'})\n",
    "target = target.rename(columns={'Target.5': 'SNS_total_target'})\n",
    "target = target.rename(columns={'Target.6': 'collection_target'})\n",
    "target = target.rename(columns={'Target.7': 'nb_focus_target'})\n",
    "target = target.rename(columns={'ERP Code': 'Erp Code'})\n",
    "target_fa_sr = target.groupby(['Erp Code'])[['FA_napkin_target', 'FA_diaper_target', 'FA_total_target']].sum().reset_index()\n",
    "sr_summary = pd.merge(sr_summary, target_fa_sr, on='Erp Code', how='left')\n",
    "sr_summary = sr_summary.rename(columns={'Net Value_x': 'Total Achivement'})\n",
    "sr_summary['Total Achivement'] = sr_summary['Total Achivement'] / 100000\n",
    "sr_summary['rrr_diff']=sr_summary['FA_total_target']-sr_summary['Total Achivement']\n",
    "\n",
    "def safe_divide(x, y):\n",
    "    try:\n",
    "        return x / y\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return 0\n",
    "        \n",
    "sr_summary['Diaper Ach%'] = sr_summary.apply(lambda row: safe_divide(row['Diaper_ach'], row['FA_diaper_target']), axis=1)\n",
    "sr_summary['Napkin Ach%'] = sr_summary.apply(lambda row: safe_divide(row['Napkin_ach'], row['FA_napkin_target']), axis=1)\n",
    "sr_summary['UTC%'] = sr_summary.apply(lambda row: safe_divide(row['UTC'], row['Count of Outlets']), axis=1)\n",
    "sr_summary['UPC%'] = sr_summary.apply(lambda row: safe_divide(row['UPC'], row['Count of Outlets']), axis=1)\n",
    "sr_summary['Average Order value'] = sr_summary.apply(lambda row: safe_divide(row['Total Achivement'], row['PC']), axis=1)\n",
    "sr_summary['LPC_value'] = sr_summary.apply(lambda row: safe_divide(row['LPC'], row['PC']), axis=1)\n",
    "sr_summary['TotalAch%'] = sr_summary.apply(lambda row: safe_divide(row['Total Achivement'], row['FA_total_target']), axis=1)\n",
    "sr_summary['DRR'] = sr_summary.apply(lambda row: safe_divide(row['Total Achivement'], number_of_working_days), axis=1)\n",
    "sr_summary['RRR'] = sr_summary.apply(lambda row: safe_divide(row['rrr_diff'], balance_days), axis=1)\n",
    "sr_summary['napkin_eco%'] = sr_summary.apply(lambda row: safe_divide(row['Napkin_eco'], row['Count of Outlets']), axis=1)\n",
    "sr_summary['diaper_eco%'] = sr_summary.apply(lambda row: safe_divide(row['Diaper_eco'], row['Count of Outlets']), axis=1)                                                        \n",
    "sr_summary['Average Order value'] *= 100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6521bd75-2ac6-446f-97d4-2517724865d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sr_summary['Diaper_ach'] = sr_summary['Diaper_ach'].round(2)\n",
    "sr_summary['Napkin_ach'] = sr_summary['Napkin_ach'].round(2)\n",
    "sr_summary['Napkin Ach%'] = sr_summary['Napkin Ach%'].round(2)\n",
    "sr_summary['Diaper Ach%'] = sr_summary['Diaper Ach%'].round(2)\n",
    "sr_summary['UTC%'] = sr_summary['UTC%'].round(2)\n",
    "sr_summary['UPC%'] = sr_summary['UPC%'].round(2)\n",
    "sr_summary['rrr_diff'] = sr_summary['rrr_diff'].round(2)\n",
    "sr_summary['TotalAch%'] = sr_summary['TotalAch%'].round(2)\n",
    "sr_summary['DRR'] = sr_summary['DRR'].round(2)\n",
    "sr_summary['RRR'] = sr_summary['RRR'].round(2)\n",
    "sr_summary['napkin_eco%'] = sr_summary['napkin_eco%'].round(2)\n",
    "sr_summary['diaper_eco%'] = sr_summary['diaper_eco%'].round(2)\n",
    "sr_summary['Average Order value'] = sr_summary['Average Order value'].round(2)\n",
    "weekdays = count_working_days_until_today()-1\n",
    "\n",
    "sr_summary = sr_summary[sr_summary['User Status'] == 'Active']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f7b0a5e-b152-4a2d-adc9-d6d977141527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "territory_data = sr_summary.groupby(['Regions','Zone','Territory']).agg({\n",
    "    'Erp Code': 'nunique',  # Count unique salesmen\n",
    "    'Order Date_x': 'sum',\n",
    "    'Count of Outlets': 'sum' ,\n",
    "    'TC': 'sum' ,\n",
    "    'UTC': 'sum' ,\n",
    "    'PC': 'sum' ,\n",
    "    'UPC': 'sum' ,\n",
    "    'LPC_value': 'mean' ,\n",
    "    'Total Achivement': 'sum',\n",
    "    'Diaper_ach': 'sum',\n",
    "    'Napkin_ach': 'sum',\n",
    "    'Napkin_eco': 'sum',\n",
    "    'Diaper_eco': 'sum',\n",
    "    'dc_eco' : 'sum',\n",
    "    'ns_eco': 'sum',\n",
    "    'ultra_eco': 'sum',\n",
    "    'ultrapremium_eco': 'sum',\n",
    "    'DRR': 'sum' ,\n",
    "    'RRR': 'sum' \n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "target_territory = target_territory.rename(columns={'Napkin Target': 'Primary_napkin_target'})\n",
    "target_territory = target_territory.rename(columns={'Diaper Target': 'Primary_diaper_target'})\n",
    "target_territory = target_territory.rename(columns={'Total Target': 'Primary_total_target'})\n",
    "target_territory = target_territory.rename(columns={'Napkin Target.1': 'FA_napkin_target'})\n",
    "target_territory = target_territory.rename(columns={'Diaper Target.1': 'FA_diaper_target'})\n",
    "target_territory = target_territory.rename(columns={'Total Target .1': 'FA_total_target'})\n",
    "target_territory = target_territory.rename(columns={'Napkin Target.2': 'SNS_napkin_target'})\n",
    "target_territory = target_territory.rename(columns={'Diaper Target.2': 'SNS_diaper_target'})\n",
    "target_territory = target_territory.rename(columns={'Total Target .2': 'SNS_total_target'})\n",
    "\n",
    "target_territory= target_territory[['Territory','FA_napkin_target', 'FA_diaper_target',\n",
    "\n",
    "       'FA_total_target','SNS_napkin_target', 'SNS_diaper_target',\n",
    "\n",
    "       'SNS_total_target']]\n",
    "\n",
    " \n",
    "territory_data = pd.merge(territory_data, target_territory, on='Territory', how='left')\n",
    "\n",
    "def safe_divide(x, y):\n",
    "    try:\n",
    "        return x / y\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return 0\n",
    "        \n",
    "        \n",
    "territory_data['Diaper Ach%'] = territory_data.apply(lambda row: safe_divide(row['Diaper_ach'], row['FA_diaper_target']), axis=1)\n",
    "territory_data['Napkin Ach%'] = territory_data.apply(lambda row: safe_divide(row['Napkin_ach'], row['FA_napkin_target']), axis=1)\n",
    "territory_data['UTC%'] = territory_data.apply(lambda row: safe_divide(row['UTC'], row['Count of Outlets']), axis=1)\n",
    "territory_data['UPC%'] = territory_data.apply(lambda row: safe_divide(row['UPC'], row['Count of Outlets']), axis=1)\n",
    "territory_data['Average Order value'] = territory_data.apply(lambda row: safe_divide(row['Total Achivement'], row['PC']), axis=1)\n",
    "territory_data['TotalAch%'] = territory_data.apply(lambda row: safe_divide(row['Total Achivement'], row['FA_total_target']), axis=1)\n",
    "territory_data['DRR'] = territory_data.apply(lambda row: safe_divide(row['Total Achivement'], number_of_working_days), axis=1)\n",
    "territory_data['napkin_eco%'] = territory_data.apply(lambda row: safe_divide(row['Napkin_eco'], row['Count of Outlets']), axis=1)\n",
    "territory_data['diaper_eco%'] = territory_data.apply(lambda row: safe_divide(row['Diaper_eco'], row['Count of Outlets']), axis=1)                                                        \n",
    "territory_data['Average Order value'] *= 100000\n",
    "territory_data['Diaper Ach'] = territory_data['Diaper_ach'].round(2)\n",
    "territory_data['Napkin Ach'] = territory_data['Napkin_ach'].round(2)\n",
    "territory_data['Napkin Ach%'] = territory_data['Napkin Ach%'].round(2)\n",
    "territory_data['Diaper Ach%'] = territory_data['Diaper Ach%'].round(2)\n",
    "territory_data['UTC%'] = territory_data['UTC%'].round(2)\n",
    "territory_data['UPC%'] = territory_data['UPC%'].round(2)\n",
    "territory_data['TotalAch%'] = territory_data['TotalAch%'].round(2)\n",
    "territory_data['DRR'] = territory_data['DRR'].round(2)\n",
    "territory_data['RRR'] = territory_data['RRR'].round(2)\n",
    "territory_data['napkin_eco%'] = territory_data['napkin_eco%'].round(2)\n",
    "territory_data['diaper_eco%'] = territory_data['diaper_eco%'].round(2)\n",
    "territory_data['Average Order value'] = territory_data['Average Order value'].round(2)\n",
    "weekdays = count_working_days_until_today()-1\n",
    "\n",
    "\n",
    "\n",
    "zone_data = sr_summary.groupby(['Regions','Zone']).agg({\n",
    "    'Erp Code': 'nunique',  # Count unique salesmen\n",
    "    'Order Date_x': 'sum',\n",
    "    'Count of Outlets': 'sum' ,\n",
    "    'TC': 'sum' ,\n",
    "    'UTC': 'sum' ,\n",
    "    'PC': 'sum' ,\n",
    "    'UPC': 'sum' ,\n",
    "    'LPC_value': 'mean' ,\n",
    "    'Total Achivement': 'sum',\n",
    "    'Diaper_ach': 'sum',\n",
    "    'Napkin_ach': 'sum',\n",
    "    'Napkin_eco': 'sum',\n",
    "    'Diaper_eco': 'sum',\n",
    "    'dc_eco' : 'sum',\n",
    "    'ns_eco': 'sum',\n",
    "    'ultra_eco': 'sum',\n",
    "    'ultrapremium_eco': 'sum',\n",
    "    'DRR': 'sum' ,\n",
    "    'RRR': 'sum' \n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "target_zone = target_zone.rename(columns={'Target': 'FA_napkin_target'})\n",
    "target_zone = target_zone.rename(columns={'Napkin Target': 'Primary_napkin_target'})\n",
    "target_zone = target_zone.rename(columns={'Diaper Target': 'Primary_diaper_target'})\n",
    "target_zone = target_zone.rename(columns={'Total Target': 'Primary_total_target'})\n",
    "target_zone = target_zone.rename(columns={'Napkin Target.1': 'FA_napkin_target'})\n",
    "target_zone = target_zone.rename(columns={'Diaper Target.1': 'FA_diaper_target'})\n",
    "target_zone = target_zone.rename(columns={'Total Target .1': 'FA_total_target'})\n",
    "target_zone = target_zone.rename(columns={'Napkin Target.2': 'SNS_napkin_target'})\n",
    "target_zone = target_zone.rename(columns={'Diaper Target.2': 'SNS_diaper_target'})\n",
    "target_zone = target_zone.rename(columns={'Total Target .2': 'SNS_total_target'})\n",
    "\n",
    "target_zone= target_zone[['Zone','FA_napkin_target', 'FA_diaper_target',\n",
    "\n",
    "       'FA_total_target','SNS_napkin_target', 'SNS_diaper_target',\n",
    "\n",
    "       'SNS_total_target']]\n",
    "\n",
    "\n",
    "zone_data = pd.merge(zone_data, target_zone, on='Zone', how='left')\n",
    "\n",
    "\n",
    "def safe_divide(x, y):\n",
    "    try:\n",
    "        return x / y\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return 0\n",
    "        \n",
    "        \n",
    "zone_data['Diaper Ach%'] = zone_data.apply(lambda row: safe_divide(row['Diaper_ach'], row['FA_diaper_target']), axis=1)\n",
    "zone_data['Napkin Ach%'] = zone_data.apply(lambda row: safe_divide(row['Napkin_ach'], row['FA_napkin_target']), axis=1)\n",
    "zone_data['UTC%'] = zone_data.apply(lambda row: safe_divide(row['UTC'], row['Count of Outlets']), axis=1)\n",
    "zone_data['UPC%'] = zone_data.apply(lambda row: safe_divide(row['UPC'], row['Count of Outlets']), axis=1)\n",
    "zone_data['Average Order value'] = zone_data.apply(lambda row: safe_divide(row['Total Achivement'], row['PC']), axis=1)\n",
    "zone_data['TotalAch%'] = zone_data.apply(lambda row: safe_divide(row['Total Achivement'], row['FA_total_target']), axis=1)\n",
    "zone_data['DRR'] = zone_data.apply(lambda row: safe_divide(row['Total Achivement'], number_of_working_days), axis=1)\n",
    "zone_data['napkin_eco%'] = zone_data.apply(lambda row: safe_divide(row['Napkin_eco'], row['Count of Outlets']), axis=1)\n",
    "zone_data['diaper_eco%'] = zone_data.apply(lambda row: safe_divide(row['Diaper_eco'], row['Count of Outlets']), axis=1)                                                        \n",
    "zone_data['Average Order value'] *= 100000\n",
    "zone_data['Diaper Ach'] = zone_data['Diaper_ach'].round(2)\n",
    "zone_data['Napkin Ach'] = zone_data['Napkin_ach'].round(2)\n",
    "zone_data['Napkin Ach%'] = zone_data['Napkin Ach%'].round(2)\n",
    "zone_data['Diaper Ach%'] = zone_data['Diaper Ach%'].round(2)\n",
    "zone_data['UTC%'] = zone_data['UTC%'].round(2)\n",
    "zone_data['UPC%'] = zone_data['UPC%'].round(2)\n",
    "zone_data['TotalAch%'] = zone_data['TotalAch%'].round(2)\n",
    "zone_data['DRR'] = zone_data['DRR'].round(2)\n",
    "zone_data['RRR'] = zone_data['RRR'].round(2)\n",
    "zone_data['napkin_eco%'] = zone_data['napkin_eco%'].round(2)\n",
    "zone_data['diaper_eco%'] = zone_data['diaper_eco%'].round(2)\n",
    "zone_data['Average Order value'] = zone_data['Average Order value'].round(2)\n",
    "weekdays = count_working_days_until_today()-1\n",
    "\n",
    "\n",
    "\n",
    "Regions_data = sr_summary.groupby(['Regions']).agg({\n",
    "    'Erp Code': 'nunique',  # Count unique salesmen\n",
    "    'Order Date_x': 'sum',\n",
    "    'Count of Outlets': 'sum' ,\n",
    "    'TC': 'sum' ,\n",
    "    'UTC': 'sum' ,\n",
    "    'PC': 'sum' ,\n",
    "    'UPC': 'sum' ,\n",
    "    'LPC_value': 'mean' ,\n",
    "    'Total Achivement': 'sum',\n",
    "    'Diaper_ach': 'sum',\n",
    "    'Napkin_ach': 'sum',\n",
    "    'Napkin_eco': 'sum',\n",
    "    'Diaper_eco': 'sum',\n",
    "    'dc_eco' : 'sum',\n",
    "    'ns_eco': 'sum',\n",
    "    'ultra_eco': 'sum',\n",
    "    'ultrapremium_eco': 'sum',\n",
    "    'DRR': 'sum' ,\n",
    "    'RRR': 'sum' \n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "target_region = target_region.rename(columns={'Target': 'FA_napkin_target'})\n",
    "target_region = target_region.rename(columns={'Napkin Target': 'Primary_napkin_target'})\n",
    "target_region = target_region.rename(columns={'Diaper Target': 'Primary_diaper_target'})\n",
    "target_region = target_region.rename(columns={'Total Target': 'Primary_total_target'})\n",
    "target_region = target_region.rename(columns={'Napkin Target.1': 'FA_napkin_target'})\n",
    "target_region = target_region.rename(columns={'Diaper Target.1': 'FA_diaper_target'})\n",
    "target_region = target_region.rename(columns={'Total Target .1': 'FA_total_target'})\n",
    "target_region = target_region.rename(columns={'Napkin Target.2': 'SNS_napkin_target'})\n",
    "target_region = target_region.rename(columns={'Diaper Target.2': 'SNS_diaper_target'})\n",
    "target_region = target_region.rename(columns={'Total Target .2': 'SNS_total_target'})\n",
    "\n",
    "target_region= target_region[['Region','FA_napkin_target', 'FA_diaper_target',\n",
    "\n",
    "       'FA_total_target','SNS_napkin_target', 'SNS_diaper_target',\n",
    "\n",
    "       'SNS_total_target']]\n",
    "target_region = target_region.rename(columns={'Region': 'Regions'})\n",
    "target_region['Regions'] = target_region['Regions'].str.replace(' Total', '', regex=False)\n",
    " \n",
    "Regions_data = pd.merge(Regions_data, target_region, on='Regions', how='left')\n",
    "\n",
    "def safe_divide(x, y):\n",
    "    try:\n",
    "        return x / y\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return 0\n",
    "        \n",
    "        \n",
    "Regions_data['Diaper Ach%'] = Regions_data.apply(lambda row: safe_divide(row['Diaper_ach'], row['FA_diaper_target']), axis=1)\n",
    "Regions_data['Napkin Ach%'] = Regions_data.apply(lambda row: safe_divide(row['Napkin_ach'], row['FA_napkin_target']), axis=1)\n",
    "Regions_data['UTC%'] = Regions_data.apply(lambda row: safe_divide(row['UTC'], row['Count of Outlets']), axis=1)\n",
    "Regions_data['UPC%'] = Regions_data.apply(lambda row: safe_divide(row['UPC'], row['Count of Outlets']), axis=1)\n",
    "Regions_data['Average Order value'] = Regions_data.apply(lambda row: safe_divide(row['Total Achivement'], row['PC']), axis=1)\n",
    "Regions_data['TotalAch%'] = Regions_data.apply(lambda row: safe_divide(row['Total Achivement'], row['FA_total_target']), axis=1)\n",
    "Regions_data['DRR'] = Regions_data.apply(lambda row: safe_divide(row['Total Achivement'], number_of_working_days), axis=1)\n",
    "Regions_data['napkin_eco%'] = Regions_data.apply(lambda row: safe_divide(row['Napkin_eco'], row['Count of Outlets']), axis=1)\n",
    "Regions_data['diaper_eco%'] = Regions_data.apply(lambda row: safe_divide(row['Diaper_eco'], row['Count of Outlets']), axis=1)                                                        \n",
    "Regions_data['Average Order value'] *= 100000\n",
    "Regions_data['Diaper Ach'] = Regions_data['Diaper_ach'].round(2)\n",
    "Regions_data['Napkin Ach'] = Regions_data['Napkin_ach'].round(2)\n",
    "Regions_data['Napkin Ach%'] = Regions_data['Napkin Ach%'].round(2)\n",
    "Regions_data['Diaper Ach%'] = Regions_data['Diaper Ach%'].round(2)\n",
    "Regions_data['UTC%'] = Regions_data['UTC%'].round(2)\n",
    "Regions_data['UPC%'] = Regions_data['UPC%'].round(2)\n",
    "Regions_data['TotalAch%'] = Regions_data['TotalAch%'].round(2)\n",
    "Regions_data['DRR'] = Regions_data['DRR'].round(2)\n",
    "Regions_data['RRR'] = Regions_data['RRR'].round(2)\n",
    "Regions_data['napkin_eco%'] = Regions_data['napkin_eco%'].round(2)\n",
    "Regions_data['diaper_eco%'] = Regions_data['diaper_eco%'].round(2)\n",
    "Regions_data['Average Order value'] = zone_data['Average Order value'].round(2)\n",
    "weekdays = count_working_days_until_today()-1\n",
    "\n",
    "sr_summary=sr_summary[['Regions',\t'Zone',\t'Territory',\t'User',\t'Erp Code',\t\t'User Status',\t'Date of Joining','Order Date_x',\t'Count of Outlets',\t'TC',\t'UTC',\t'UTC%',\t'PC',\t'UPC',\t'UPC%',\t'LPC_value','Average Order value',\t'FA_napkin_target',\t'Napkin_ach',\t'Napkin Ach%',\t'FA_diaper_target',\t'Diaper_ach',\t'Diaper Ach%',\t'FA_total_target',\t'Total Achivement',\t'TotalAch%',\t'DRR',\t'RRR',\t'Napkin_eco',\t'napkin_eco%',\t'Diaper_eco',\t'diaper_eco%',\t'dc_eco',\t'ns_eco',\t'ultra_eco',\t'ultrapremium_eco']]\n",
    "territory_data=territory_data[['Regions',\t'Zone',\t'Territory',\t'Erp Code',\t'Order Date_x',\t'Count of Outlets',\t'TC',\t'UTC',\t'UTC%',\t'PC',\t'UPC',\t'UPC%',\t'LPC_value','Average Order value',\t'FA_napkin_target','SNS_napkin_target',\t'Napkin Ach',\t'Napkin Ach%',\t'FA_diaper_target','SNS_diaper_target',\t'Diaper_ach',\t'Diaper Ach%',\t'FA_total_target','SNS_total_target','Total Achivement',\t'TotalAch%',\t'DRR',\t'RRR',\t'Napkin_eco',\t'napkin_eco%',\t'Diaper_eco',\t'diaper_eco%',\t'dc_eco',\t'ns_eco',\t'ultra_eco',\t'ultrapremium_eco']]\n",
    "zone_data=zone_data[['Regions',\t'Zone',\t'Erp Code',\t'Order Date_x',\t'Count of Outlets',\t'TC',\t'UTC',\t'UTC%',\t'PC',\t'UPC',\t'UPC%','LPC_value',\t'Average Order value',\t'FA_napkin_target',\t'SNS_napkin_target','Napkin Ach',\t'Napkin Ach%',\t'FA_diaper_target','SNS_diaper_target',\t'Diaper_ach',\t'Diaper Ach%',\t'FA_total_target','SNS_total_target',\t'Total Achivement',\t'TotalAch%',\t'DRR',\t'RRR','Napkin_eco',\t'napkin_eco%',\t'Diaper_eco',\t'diaper_eco%',\t'dc_eco',\t'ns_eco',\t'ultra_eco',\t'ultrapremium_eco']]\n",
    "Regions_data=Regions_data[['Regions',\t'Erp Code',\t'Order Date_x',\t'Count of Outlets',\t'TC',\t'UTC',\t'UTC%',\t'PC',\t'UPC',\t'UPC%','LPC_value',\t'Average Order value',\t'FA_napkin_target'\t,'SNS_napkin_target',\t'Napkin Ach',\t'Napkin Ach%',\t'FA_diaper_target','SNS_diaper_target',\t'Diaper_ach',\t'Diaper Ach%',\t'FA_total_target','SNS_total_target',\t'Total Achivement',\t'TotalAch%',\t'DRR',\t'RRR','Napkin_eco',\t'napkin_eco%',\t'Diaper_eco',\t'diaper_eco%',\t'dc_eco',\t'ns_eco',\t'ultra_eco',\t'ultrapremium_eco']]\n",
    "region_eco=Regions_data[['Regions',\t'Erp Code',\t'Count of Outlets','Napkin_eco',\t'napkin_eco%',\t'Diaper_eco',\t'diaper_eco%',\t'dc_eco',\t'ns_eco',\t'ultra_eco',\t'ultrapremium_eco']]\n",
    "zone_eco=zone_data[['Regions',\t'Zone',\t'Erp Code',\t'Count of Outlets',\t'Napkin_eco',\t'napkin_eco%',\t'Diaper_eco',\t'diaper_eco%',\t'dc_eco',\t'ns_eco',\t'ultra_eco',\t'ultrapremium_eco']]\n",
    "beat_summary=beat_summary[['Regions',\t'Zone',\t'Territory',\t'User',\t'Erp Code',\t\t'Beat Erp Id',\t'Beats','Order Date',\t'Count of Outlets',\t'TC',\t'UTC',\t'UTC%',\t'PC',\t'UPC',\t'UPC%',\t'Average Order value',\t'Napkin_ach',\t'Diaper_ach',\t'Net Value',\t'DRR',\t\t'Napkin_eco',\t'napkin_eco%',\t'Diaper_eco',\t'diaper_eco%',\t'dc_eco',\t'ns_eco',\t'ultra_eco',\t'ultrapremium_eco']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e29b1-32b6-452f-b8a1-100769458a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a6a1942-f710-43cd-9b83-d50d29237e39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved successfully at C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\images\\Region.png\n",
      "Image saved successfully at C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\images\\Region_eco.png\n",
      "Image saved successfully at C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\images\\Zone.png\n",
      "Image saved successfully at C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\images\\zone_eco.png\n"
     ]
    }
   ],
   "source": [
    "import xlwings as xw\n",
    "workbook=xw.Book(r'C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\SR Daily Performance Tracker Automated.xlsx') \n",
    "region = xw.sheets('Region summary') \n",
    "zone = xw.sheets('Zone summary')\n",
    "terrirtory = xw.sheets('Territory Summary')\n",
    "sr_summary1 = xw.sheets('SR Summary')\n",
    "beat_summary1= xw.sheets('Beat Summary')\n",
    "\n",
    "sr_summary=sr_summary.reset_index() \n",
    "territory_data= territory_data.reset_index() \n",
    "zone_data= zone_data.reset_index() \n",
    "Regions_data=Regions_data.reset_index()\n",
    "beat_summary2=beat_summary.reset_index()\n",
    "zone_eco= zone_eco.reset_index() \n",
    "region_eco=region_eco.reset_index()\n",
    "\n",
    "sr_summary1.range('A6:BP500000').value = \"\"\n",
    "terrirtory.range('A6:BP500000').value = \"\"\n",
    "zone.range('A6:BP500000').value = \"\"\n",
    "region.range('A6:BP500000').value = \"\"\n",
    "beat_summary1.range('A6:BP500000').value = \"\"\n",
    "\n",
    "sr_summary1.range('A6').value = [sr_summary.columns.tolist()]\n",
    "sr_summary1.range('A6').value = sr_summary.values\n",
    "\n",
    "sr_summary1.range('A6').value = sr_summary.values\n",
    "terrirtory.range('A6').value = [territory_data.columns.tolist()]\n",
    "terrirtory.range('A6').value = territory_data.values\n",
    "zone.range('A6').value = [zone_data.columns.tolist()]\n",
    "zone.range('A6').value = zone_data.values\n",
    "region.range('A6').value = [Regions_data.columns.tolist()]\n",
    "region.range('A6').value = Regions_data.values\n",
    "\n",
    "zone.range('AQ6').value = [zone_eco.columns.tolist()]\n",
    "zone.range('AQ6').value = zone_eco.values\n",
    "region.range('AQ6').value = [region_eco.columns.tolist()]\n",
    "region.range('AQ6').value = region_eco.values\n",
    "\n",
    "beat_summary1.range('A6').value = [beat_summary2.columns.tolist()]\n",
    "beat_summary1.range('A6').value = beat_summary2.values\n",
    "workbook.api.RefreshAll()\n",
    "\n",
    "\n",
    "workbook.save()\n",
    "\n",
    "import xlwings as xw\n",
    "from PIL import ImageGrab\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Define the path where the image will be saved\n",
    "path = r'C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\images'\n",
    "\n",
    "# Open the workbook and select the sheet\n",
    "workbook_path = r'C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\SR Daily Performance Tracker Automated.xlsx'\n",
    "workbook = xw.Book(workbook_path)\n",
    "sheet = workbook.sheets['Region Summary']\n",
    "\n",
    "# Define the range to copy\n",
    "copy_range = \"A1:AA13\"\n",
    "\n",
    "def export_range_as_image(sheet, copy_range, image_name, save_path):\n",
    "    # Select the range\n",
    "    rng = sheet.range(copy_range)\n",
    "    \n",
    "    # Copy the range as a picture\n",
    "    rng.api.CopyPicture(Appearance=1, Format=2)\n",
    "    \n",
    "    # Grab the image from the clipboard\n",
    "    img = ImageGrab.grabclipboard()\n",
    "    \n",
    "    # Check if the image was successfully grabbed\n",
    "    if img:\n",
    "        # Define the full path for the image\n",
    "        full_image_path = os.path.join(save_path, f\"{image_name}.png\")\n",
    "        \n",
    "        # Save the image\n",
    "        img.save(full_image_path)\n",
    "        print(f\"Image saved successfully at {full_image_path}\")\n",
    "    else:\n",
    "        print(\"No image found in clipboard\")\n",
    "\n",
    "# Call the function to export the image\n",
    "export_range_as_image(sheet, copy_range, \"Region\", path)\n",
    "\n",
    "\n",
    "copy_range = \"AQ1:BI13\"\n",
    "\n",
    "def export_range_as_image(sheet, copy_range, image_name, save_path):\n",
    "    # Select the range\n",
    "    rng = sheet.range(copy_range)\n",
    "    \n",
    "    # Copy the range as a picture\n",
    "    rng.api.CopyPicture(Appearance=1, Format=2)\n",
    "    \n",
    "    # Grab the image from the clipboard\n",
    "    img = ImageGrab.grabclipboard()\n",
    "    \n",
    "    # Check if the image was successfully grabbed\n",
    "    if img:\n",
    "        # Define the full path for the image\n",
    "        full_image_path = os.path.join(save_path, f\"{image_name}.png\")\n",
    "        \n",
    "        # Save the image\n",
    "        img.save(full_image_path)\n",
    "        print(f\"Image saved successfully at {full_image_path}\")\n",
    "    else:\n",
    "        print(\"No image found in clipboard\")\n",
    "\n",
    "# Call the function to export the image\n",
    "export_range_as_image(sheet, copy_range, \"Region_eco\", path)\n",
    "\n",
    "\n",
    "sheet = workbook.sheets['Zone Summary']\n",
    "\n",
    "# Define the range to copy\n",
    "copy_range = \"A1:AB32\"\n",
    "\n",
    "def export_range_as_image(sheet, copy_range, image_name, save_path):\n",
    "    # Select the range\n",
    "    rng = sheet.range(copy_range)\n",
    "    \n",
    "    # Copy the range as a picture\n",
    "    rng.api.CopyPicture(Appearance=1, Format=2)\n",
    "    \n",
    "    # Grab the image from the clipboard\n",
    "    img = ImageGrab.grabclipboard()\n",
    "    \n",
    "    # Check if the image was successfully grabbed\n",
    "    if img:\n",
    "        # Define the full path for the image\n",
    "        full_image_path = os.path.join(save_path, f\"{image_name}.png\")\n",
    "        \n",
    "        # Save the image\n",
    "        img.save(full_image_path)\n",
    "        print(f\"Image saved successfully at {full_image_path}\")\n",
    "    else:\n",
    "        print(\"No image found in clipboard\")\n",
    "\n",
    "# Call the function to export the image\n",
    "export_range_as_image(sheet, copy_range, \"Zone\", path)\n",
    "\n",
    "copy_range = \"AQ1:BI32\"\n",
    "\n",
    "def export_range_as_image(sheet, copy_range, image_name, save_path):\n",
    "    # Select the range\n",
    "    rng = sheet.range(copy_range)\n",
    "    \n",
    "    # Copy the range as a picture\n",
    "    rng.api.CopyPicture(Appearance=1, Format=2)\n",
    "    \n",
    "    # Grab the image from the clipboard\n",
    "    img = ImageGrab.grabclipboard()\n",
    "    \n",
    "    # Check if the image was successfully grabbed\n",
    "    if img:\n",
    "        # Define the full path for the image\n",
    "        full_image_path = os.path.join(save_path, f\"{image_name}.png\")\n",
    "        \n",
    "        # Save the image\n",
    "        img.save(full_image_path)\n",
    "        print(f\"Image saved successfully at {full_image_path}\")\n",
    "    else:\n",
    "        print(\"No image found in clipboard\")\n",
    "\n",
    "# Call the function to export the image\n",
    "export_range_as_image(sheet, copy_range, \"zone_eco\", path)\n",
    "\n",
    "\n",
    "# Introduce a delay to ensure the image is saved\n",
    "time.sleep(3)\n",
    "\n",
    "# Close the workbook without saving changes\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00060255-2617-4483-9526-7fcc8e31897f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending Mail\n",
      "SR daily tracker Sent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import smtplib\n",
    "from email.mime.image import MIMEImage\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "import mimetypes\n",
    "import os\n",
    "from datetime import datetime\n",
    " \n",
    "# Get today's date\n",
    "today_date = datetime.now().strftime('%Y-%m-%d')\n",
    " \n",
    "# Email components\n",
    "region = '<img src=\"cid:Region\"></img><br/><br/>'\n",
    "zone = '<img src=\"cid:Zone\"></img><br/><br/>'\n",
    "region_eco = '<img src=\"cid:Region_eco\"></img><br/><br/>'\n",
    "zone_eco = '<img src=\"cid:zone_eco\"></img><br/><br/>'\n",
    "\n",
    "Image1 = '<img src=\"cid:region\"></img><br/><br/>'\n",
    "Image2 = '<img src=\"cid:region_eco\"></img><br/><br/>'\n",
    "Image3 = '<img src=\"cid:zone\"></img><br/><br/>'\n",
    "Image4 = '<img src=\"cid:zone_eco\"></img><br/><br/>'\n",
    " \n",
    "text0 = \"<div style='color:red'>  </div> <br/>\"\n",
    "text1 = \"Hi All,<br/> <br/> PFA SR Daily Performance Tracker.<br/><br/>\"\n",
    "text2 = \"Note:<br/> <br/> 1. This tracker is for current active SRs.<br/> <br/> 2. For clarity, targets at Region, Zone and Territory level are same as shared in final target file, irrespective of SRs.<br/><br/>\"\n",
    "regards = '<br/> <br/> <br/> <br/> Regards <br/> Niine Analytics Team'\n",
    "body = text0 + text1 + text2 + region + region_eco + zone + zone_eco + regards\n",
    " \n",
    "subject = f'SR Daily Performance Tracker|| Auto || {today_date}'\n",
    " \n",
    "rec_To_list = [\n",
    "'rohit.shahi@niine.com',\n",
    "'manoj.mishra@niine.com',\n",
    "'kuldeep.singh@niine.com',\n",
    "'sanjay.kumar@niine.com',\n",
    "'rupal.agarwal@niine.com',\n",
    "'gopal.prasad1@niine.com',\n",
    "'gaurav.seth@niine.com',\n",
    "'sachin.bhalme@niine.com',\n",
    "'ajay.maan@niine.com',\n",
    "'vikas.singh@niine.com',\n",
    "'ravi.nigam@niine.com',\n",
    "'prakhar.shukla@niine.com',\n",
    "'kapil.tyagi@niine.com',\n",
    "'mohamed.noor@niine.com',\n",
    "'pankaj.bhagat@niine.com',\n",
    "'ram.sanehi@niine.com',\n",
    "'sarabjeet.singh@niine.com',\n",
    "'g.srikanth@niine.com',\n",
    "'amit.das@niine.com',\n",
    "'amit.kumar@niine.com',\n",
    "'anutosh.ranjan@niine.com',\n",
    "'dileep.dubey@niine.com',\n",
    "'vinay.thacker@niine.com',\n",
    "'gedela.srinivas@niine.com',\n",
    "'avinshtiwari@niine.com',\n",
    "'yugank.mishra@niine.com',\n",
    "'abhishek.das03@niine.com',\n",
    "'udeshya.gupta@niine.com',\n",
    "'surya.prakash@niine.com',\n",
    "'kalu@niine.com',\n",
    "'deepak.patil@niine.com',\n",
    "'kaushal.kumar1@niine.com',\n",
    "'santosh.dhal@niine.com',\n",
    "'rajkumar.halder@niine.com',\n",
    "'praloy.sen@niine.com',\n",
    "'pankaj.jha@niine.com',\n",
    "'pawan.sharma@niine.com',\n",
    "'ajay.jha@niine.com',\n",
    "'amit.k@niine.com',\n",
    "'santanu.sen@niine.com',\n",
    "'satyapal.sharma@niine.com',\n",
    "'ketan.vaidya@niine.com',\n",
    "'amitava.ghosh@niine.com',\n",
    "'saikat.chakraborty@niine.com',\n",
    "'purushottam.mishra@niine.com',\n",
    "'motiraj.tamang@niine.com',\n",
    "'pankaj.thakur@niine.com',\n",
    "'sandeep.singh@niine.com',\n",
    "'umesh.upmanyu@niine.com',\n",
    "'priyank.roy@niine.com',\n",
    "'bhupesh.mishra@niine.com',\n",
    "'arpan.mondal@niine.com',\n",
    "'op.pandey@niine.com ',\n",
    "'shankar.gurnani@niine.com  ',\n",
    "'shashikant.kori@niine.com',\n",
    "'saurabh.kumar@niine.com',\n",
    "'anurag.singh@niine.com',\n",
    "'bhikam.singh@niine.com ',\n",
    "'tripurari.tripathi@niine.com',\n",
    "'koushik.roy@niine.com',\n",
    "'ashish.kumar@niine.com',\n",
    "'dhananjay.joshi@niine.com',\n",
    "'ashim.saha@niine.com',\n",
    "'naseem.jawaid@niine.com',\n",
    "'sujith.sripadha@niine.com',\n",
    "'hitesh.sahu@niine.com',\n",
    "'joydeep.biswas@niine.com',\n",
    "'kulveer.rastogi@niine.com',\n",
    "'sudip.banerjee@niine.com',\n",
    "'ganpat.rathore@niine.com',\n",
    "'vikas.jayaswal@niine.com',\n",
    "'ratnesh.dwivedi@niine.com',\n",
    "'anil.mishra@niine.com',\n",
    "'basant.singh@niine.com',\n",
    "'jitendra.yadav@niine.com',\n",
    "'bhupesh@niine.com ',\n",
    "'dhyanpal.singh@niine.com',\n",
    "'vishal.bhan@niine.com',\n",
    "'ravi.ranjan@niine.com',\n",
    "'niraj.upadhyay@niine.com',\n",
    "'prasanta.mitra@niine.com',\n",
    "'suresh.kumar@niine.com',\n",
    "'tanvir.pathan@niine.com',\n",
    "'prasun.bhowmik@niine.com',\n",
    "'hamid.ansari@niine.com',\n",
    "'lokendra.singh@niine.com',\n",
    "'shailendra.kumar@niine.com',\n",
    "'Agastya.singh@niine.com',\n",
    "'Shreemala.Dhayal@niine.com'\n",
    "]\n",
    "to_email_list = ', '.join(rec_To_list)\n",
    " \n",
    "rec_CC_list = [ \n",
    "   'ameya.dangi@niine.com','Shreemala.Dhayal@niine.com','gaurav@niine.com','krishan.malav@niine.com','Akshay.Sundareshan@niine.com','Agastya.singh@niine.com'\n",
    "]\n",
    "to_CC_list = ', '.join(rec_CC_list)\n",
    " \n",
    "# File and image paths\n",
    "filename = r\"C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\SR Daily Performance Tracker Automated.xlsx\"\n",
    "image_path = r\"C:\\Users\\Agastya Singh\\OneDrive - Niine Pvt. Ltd\\Desktop\\SR Daily tracker\\images\" # Update with the actual path to the images\n",
    " \n",
    "def send_email(subject, body, to_email_list, filename):\n",
    "    to_emails = to_email_list\n",
    "    to_CC = to_CC_list\n",
    "    message = body\n",
    " \n",
    "    # Set up the SMTP server\n",
    "    s = smtplib.SMTP(host='smtp.outlook.office365.com', port=587) \n",
    "    s.starttls()\n",
    "    s.login('ram.kishor@niine.com', 'Niine@2024')  # Use environment variable or secure method\n",
    " \n",
    "    # Create the email\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = 'ram.kishor@niine.com'\n",
    "    msg['To'] = to_emails\n",
    "    msg['CC'] = to_CC\n",
    "    msg['Subject'] = subject\n",
    "    msg.attach(MIMEText(message, 'html'))\n",
    " \n",
    "    def attach_image(image_name):\n",
    "        with open(os.path.join(image_path, image_name + '.png'), 'rb') as fp:\n",
    "            msg_image = MIMEImage(fp.read())\n",
    "            msg_image.add_header('Content-ID', f'<{image_name}>')\n",
    "            msg.attach(msg_image)\n",
    " \n",
    "    # Attach images\n",
    "    attach_image('region')\n",
    "    attach_image('region_eco')\n",
    "    attach_image('zone')\n",
    "    attach_image('zone_eco')\n",
    " \n",
    "    # Attach file\n",
    "    ctype, encoding = mimetypes.guess_type(filename)\n",
    "    if ctype is None or encoding is not None:\n",
    "        ctype = \"application/octet-stream\"\n",
    "    maintype, subtype = ctype.split(\"/\", 1)\n",
    "    with open(filename, \"rb\") as fp:\n",
    "        attachment = MIMEBase(maintype, subtype)\n",
    "        attachment.set_payload(fp.read())\n",
    "        encoders.encode_base64(attachment)\n",
    "        attachment.add_header(\"Content-Disposition\", \"attachment\", filename=os.path.basename(filename))\n",
    "        msg.attach(attachment)\n",
    " \n",
    "    # Send the email\n",
    "    s.send_message(msg)\n",
    "    s.quit()\n",
    " \n",
    "print('Sending Mail')\n",
    "send_email(subject, body, to_email_list, filename)\n",
    "print('SR daily tracker Sent')\n",
    " \n",
    "# Optional: Close any open Excel processes\n",
    "os.system(\"TASKKILL /F /IM excel.exe\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
